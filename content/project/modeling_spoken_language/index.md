---
title: Modeling Spoken Language Beyond Text
summary: 
tags:
- Featured
- Speech & Cognition
date: "2025-10-01T00:00:00Z"
authors:
- rohanianmorteza

active: true

# Optional external URL for project (replaces project detail page).
# external_link: ""

image:
  caption: 
  focal_point: Smart

links:
#- icon: github
#  icon_pack: fab
#  name: View Code
#  url: https://github.com/uzh-dqbm-cmi/template

url_code: ""
url_pdf: "https://arxiv.org/pdf/2509.26276"
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

Audio-first approaches to understanding language directly from speech
We develop models that learn language directly from speech rather than text. By focusing on acoustic consistency and capturing prosody, disfluencies, and non-verbal cues, our work aims to make spoken language understanding more natural and representative of how people communicate across different languages and speakers. These approaches open paths toward richer humanâ€“AI interaction and future applications in domains such as clinical communication, where vocal nuance carries critical context.
We also study language models for resource-constrained devices, enabling efficient and accessible AI that can operate on mobile and edge systems. (read more here: https://arxiv.org/abs/2509.26276)
