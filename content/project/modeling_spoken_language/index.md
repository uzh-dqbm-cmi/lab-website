---
title: Modeling Spoken Language Beyond Text
summary: Audio-first approaches to understanding language directly from speech
tags:
- Featured
- Speech and Cognition
date: "2025-10-01T00:00:00Z"
authors:
- rohanianmorteza


# Optional external URL for project (replaces project detail page).
# external_link: ""

image:
  caption: 
  focal_point: Smart

links:
#- icon: github
#  icon_pack: fab
#  name: View Code
#  url: https://github.com/uzh-dqbm-cmi/template

url_code: ""
url_pdf: "https://arxiv.org/pdf/2509.26276"
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

We develop models that learn language directly from speech rather than text. By focusing on acoustic consistency and capturing prosody, disfluencies, and non-verbal cues, our work aims to make spoken language understanding more natural and representative of how people communicate across different languages and speakers. These approaches open paths toward richer humanâ€“AI interaction and future applications in domains such as clinical communication, where vocal nuance carries critical context.
We also study language models for resource-constrained devices, enabling efficient and accessible AI that can operate on mobile and edge systems. (read more here: [click here](https://arxiv.org/abs/2509.26276))
